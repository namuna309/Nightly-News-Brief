import re
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from webdriver_manager.chrome import ChromeDriverManager

class LinksScraper:
    def __init__(self, url: str):
        self.url = url
        # Selenium ì˜µì…˜ ì„¤ì • (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ)
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ
        options.add_argument('--ignore-certificate-errors')
        options.add_argument('--ignore-certificate-errors-spki-list')
        options.add_argument('--ignore-ssl-errors')
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    def scroll_down_until_yesterday(self, max_scrolls=20, wait_time=2):
        """
        ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¬ë©´ì„œ 'publishing' í´ë˜ìŠ¤ë¥¼ í¬í•¨í•œ íƒœê·¸ ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ë¥¼ í™•ì¸.
        ë‚ ì§œ ì •ë³´ì— ìˆ«ìê°€ í¬í•¨ë˜ì§€ ì•Šìœ¼ë©´ ìŠ¤í¬ë¡¤ì„ ì¤‘ë‹¨í•˜ë©°,
        ìµœëŒ€ max_scrolls íšŸìˆ˜ê¹Œì§€ ìŠ¤í¬ë¡¤í•©ë‹ˆë‹¤.
        """
        previous_date_texts = []
        for scroll in range(max_scrolls):
            time.sleep(wait_time)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            try:
                date_elements = self.driver.find_elements(By.XPATH, "//div[contains(@class, 'publishing')]")
                date_texts = [el.get_attribute("textContent").split("â€¢")[-1].strip() for el in date_elements]
            except Exception as e:
                print("âŒ XPath ì˜¤ë¥˜ ë°œìƒ:", e)
                return

            # ìƒˆë¡œ ë¡œë“œëœ ë‚ ì§œ í…ìŠ¤íŠ¸ í™•ì¸
            new_date_texts = date_texts[len(previous_date_texts):]

            # ë‚ ì§œ ì •ë³´ì— ìˆ«ìê°€ ì—†ëŠ” ê²½ìš° (ex. "yesterday" ë“±) ìŠ¤í¬ë¡¤ ì¤‘ë‹¨
            contains_number = any(re.search(r'\d', text) for text in new_date_texts)
            if not contains_number:
                return

            previous_date_texts.extend(new_date_texts)
            # í˜ì´ì§€ í•˜ë‹¨ìœ¼ë¡œ ìŠ¤í¬ë¡¤
            self.driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
            # print(f"â¬‡ ìŠ¤í¬ë¡¤ {scroll + 1}íšŒ ì‹¤í–‰")

        # print("ğŸš¨ ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜ ë„ë‹¬, ì¤‘ì§€")

    def extract_article_links(self) -> list:
        """
        - 'content' í´ë˜ìŠ¤ë¥¼ í¬í•¨í•œ div ìš”ì†Œë“¤ì„ ì°¾ê³ ,
        - ë‚´ë¶€ì˜ footer > publishing ì˜ì—­ì—ì„œ ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ í™•ì¸í•˜ì—¬,
        - ë‚ ì§œê°€ "yesterday"ê°€ ì•„ë‹Œ ê²½ìš° í•´ë‹¹ content ë‚´ë¶€ì˜ a íƒœê·¸ hrefë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        article_links = []
        try:
            content_divs = self.driver.find_elements(By.XPATH, "//div[contains(@class, 'content')]")
        except Exception as e:
            print("âŒ content ìš”ì†Œ ì°¾ê¸° ì‹¤íŒ¨:", e)
            return article_links

        for content in content_divs:
            try:
                # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ (footer > publishing)
                date_element = content.find_element(
                    By.XPATH, ".//div[contains(@class, 'footer')]//div[contains(@class, 'publishing')]"
                )
                date_text = date_element.get_attribute("textContent").split("â€¢")[-1].strip()
            except Exception:
                continue  # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ

            # ì›ë˜ ì½”ë“œì˜ ë¡œì§: ë‚ ì§œ ì •ë³´ê°€ "yesterday"ê°€ ì•„ë‹Œ ê²½ìš° ë§í¬ ì¶”ì¶œ
            if "yesterday" != date_text:
                try:
                    article_link = content.find_element(By.XPATH, ".//a").get_attribute("href")
                    article_links.append(article_link)
                except Exception:
                    continue

        return article_links

    def get_article_links(self) -> list:
        """
        ì…ë ¥ URLë¡œ ì ‘ì† í›„, ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ ë‚ ì§œ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ ,
        ê¸°ì‚¬ ë§í¬(article_links)ë¥¼ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        
        for i in range(5):
            self.driver.get(self.url)
            try:
                WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, "publishing")))
            except:
                print(f'{self.url}ì›¹í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨. ì¬ì‹œë„ ì‹¤í–‰')
                continue
            else:
                time.sleep(3)  # í˜ì´ì§€ ì´ˆê¸° ë¡œë”© ëŒ€ê¸°
                self.scroll_down_until_yesterday(max_scrolls=20, wait_time=2)
                links = self.extract_article_links()
                self.driver.quit()
                return links
            

# ì‚¬ìš© ì˜ˆì œ:
if __name__ == "__main__":
    URL = "https://finance.yahoo.com/topic/stock-market-news/"
    scraper = LinksScraper(URL)
    article_links = scraper.get_article_links()
    if article_links:
        print("ğŸ”¹ ì˜¤ëŠ˜ ê¸°ì‚¬ ë§í¬:")
        for link in article_links:
            print(link)
    else:
        print("ğŸš¨ 'yesterday'ì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ ë§í¬ ì—†ìŒ.")
