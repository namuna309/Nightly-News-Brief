import os
import json
import sys
import pyspark.pandas as ps
from datetime import datetime
from zoneinfo import ZoneInfo
from bs4 import BeautifulSoup
from pyspark.sql import SparkSession
import concurrent.futures
import warnings
from pyspark.conf import SparkConf
from urllib.parse import unquote
import boto3
from dotenv import load_dotenv

load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (PyArrow ì˜¤ë¥˜ ë°©ì§€)
AWS_ACCESS_KEY = unquote(os.environ.get('AWS_ACCESS_KEY'))  # AWS ì•¡ì„¸ìŠ¤ í‚¤ ë¡œë“œ
AWS_SECRET_KEY = unquote(os.environ.get('AWS_SECRET_KEY'))  # AWS ì‹œí¬ë¦¿ í‚¤ ë¡œë“œ
S3_REGION = unquote(os.environ.get('S3_REGION'))  # AWS ì‹œí¬ë¦¿ í‚¤ ë¡œë“œ
BUCKET_NAME = unquote(os.environ.get('BUCKET_NAME')) 
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

s3_client = boto3.client(  # S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    service_name='s3',
    aws_access_key_id=AWS_ACCESS_KEY,
    aws_secret_access_key=AWS_SECRET_KEY,
    region_name=S3_REGION
)

conf = SparkConf()
conf.set('spark.driver.host', '127.0.0.1')  # Spark ë“œë¼ì´ë²„ í˜¸ìŠ¤íŠ¸ ì„¤ì •
conf.set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)  # S3 ì•¡ì„¸ìŠ¤ í‚¤ ì„¤ì •
conf.set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)  # S3 ì‹œí¬ë¦¿ í‚¤ ì„¤ì •
conf.set("spark.jars.packages", 'org.apache.hadoop:hadoop-aws:3.3.4')  # S3 íŒ¨í‚¤ì§€ ì„¤ì •


# SparkSession ìƒì„±
spark = SparkSession.builder.config(conf=conf).master("local[4]").appName("JSON_to_Parquet").getOrCreate()

# í˜„ì¬ ë‚ ì§œ (Eastern Time ê¸°ì¤€)
today = datetime.now(ZoneInfo("America/New_York")).date()

# í…Œë§ˆ ì„¤ì •
THEMES = ['Stock_Market', 'Original', 'Economies', 'Earning', 'Tech', 'Housing', 'Crypto']


def get_prefix(data_stage, theme):
    """í…Œë§ˆë³„ JSON ë° Parquet ì €ì¥ ê²½ë¡œ ë°˜í™˜"""
    return f"{data_stage}/ARTICLES/{theme.upper()}/year={today.year}/month={today.strftime('%m')}/day={today.strftime('%d')}"

def load_json_files():
    """í…Œë§ˆë³„ JSON íŒŒì¼ ê²½ë¡œ ë¡œë“œ"""
    try:
        url_per_themes = {theme: [] for theme in THEMES}
        for theme in THEMES:
            prefix = get_prefix('RAW', theme)
            response = s3_client.list_objects_v2(Bucket=BUCKET_NAME, Prefix=prefix)
            if "Contents" in response:
                for obj in response["Contents"]:
                    if obj["Key"].endswith(".json"):  # JSON íŒŒì¼ë§Œ í•„í„°ë§
                        url_per_themes[theme].append(f"{obj['Key']}")
    except Exception as e:
        print(f"prefix ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    else:
        print("jsoníŒŒì¼ s3 prfix ì¶”ì¶œ ì™„ë£Œ")
        return url_per_themes

def extract_article_data(file_path):
    """JSON íŒŒì¼ì—ì„œ HTML íŒŒì‹± ë° ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ"""
    object = s3_client.get_object(Bucket=BUCKET_NAME, Key=file_path)
    print(f"{file_path}: s3 object read ì™„ë£Œ")
    article = json.loads(object['Body'].read())
    print(f"{file_path}: object -> json ì¶”ì¶œ ì™„ë£Œ")
    soup = BeautifulSoup(article['content'], "html.parser")
    article_data = {
        'title': '',
        'date': '',
        'authors': [],
        'url': article['url'],
        'text': ''
    }

    article_wrap = soup.find("div", class_=lambda c: c and "article-wrap" in c)
    if not article_wrap:
        print(f"'article-wrap' ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URL: {article['url']}")
        return article_data

    # ì œëª© ì¶”ì¶œ
    cover_title = article_wrap.find("div", class_=lambda c: c and "cover-title" in c)
    article_data['title'] = cover_title.get_text(strip=True) if cover_title else ""

    # ì €ì ì¶”ì¶œ
    byline_author = article_wrap.find("div", class_=lambda c: c and "byline-attr-author" in c)
    if byline_author:
        try:
            author_links = byline_author.find_all("a")
            if author_links:
                article_data['authors'] = [a.get_text(strip=True) for a in author_links]
                if article_data['authors'] == ['']:
                    author_img = author_links[0].find('img')
                    article_data['authors'] = [author_img['alt'].split(', ')[0]]
            else:
                authors_text = byline_author.text
                article_data['authors'] = [author.strip() for author in authors_text.split(', ')] if ',' in authors_text else [authors_text.strip()]
        except Exception as e:
            print(f"âŒ 'authors' íŒŒì‹± ì‹¤íŒ¨: {e}. URL: {article_data['url']}")

    # ë‚ ì§œ ì¶”ì¶œ
    meta_time = article_wrap.find("time", class_=lambda c: c and "byline-attr-meta-time" in c)
    article_data['date'] = meta_time.get("data-timestamp", "") if meta_time else ""

    # ë³¸ë¬¸ ì¶”ì¶œ
    body_wrap = soup.find("div", class_=lambda c: c and "body-wrap" in c)
    if body_wrap:
        body_div = body_wrap.find("div", class_=lambda c: c and "body" in c)
        article_data['text'] = body_div.get_text(separator="\n", strip=True) if body_div else ""

    print(f"{article_data['url']} ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
    return article_data

def save_to_s3(article_list, theme):
    """ê¸°ì‚¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ Parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥ (pandas-on-Spark ì‚¬ìš©)"""
    df = ps.DataFrame(article_list)  # pandas-on-Spark DataFrame ìƒì„±
    spark_df = df.to_spark()
    spark_df = spark_df.coalesce(1)
    prefix = get_prefix('TRANSFORMED', theme)

    spark_df.write.option('header', 'true').mode('overwrite').parquet(f's3a://{BUCKET_NAME}/{prefix}')
    print(f"ğŸ¯ Parquet ì €ì¥ ì™„ë£Œ: f's3a://{BUCKET_NAME}/{prefix}'")

def process_articles(url_per_themes):
    """í…Œë§ˆë³„ JSON íŒŒì¼ì„ ì²˜ë¦¬ í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥ (ë³‘ë ¬ ì²˜ë¦¬)"""
    for theme, file_paths in url_per_themes.items():
        article_list = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = {executor.submit(extract_article_data, file_path): file_path for file_path in file_paths}
            for future in concurrent.futures.as_completed(futures):
                try:
                    article_data = future.result()
                    article_list.append(article_data)
                except Exception as e:
                    print(f"ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        if article_list:
            save_to_s3(article_list, theme)

if __name__ == "__main__":
    import time
    start = time.time()
    file_paths = load_json_files()
    process_articles(file_paths)
    print('time: ', time.time() - start)
    spark.stop()  # SparkSession ì¢…ë£Œ