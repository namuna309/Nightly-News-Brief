import os
import json
import glob
import pandas as pd
from datetime import datetime
from zoneinfo import ZoneInfo
from bs4 import BeautifulSoup
import concurrent.futures


# âœ… í˜„ì¬ ë‚ ì§œ (Eastern Time ê¸°ì¤€)
today = datetime.now(ZoneInfo("America/New_York")).date()

# âœ… í…Œë§ˆ ì„¤ì •
THEMES = ['Stock_Market', 'Original', 'Economies', 'Earning', 'Tech', 'Housing', 'Crypto']

def get_folder_path(base_dir, theme):
    """í…Œë§ˆë³„ JSON ë° Parquet ì €ì¥ ê²½ë¡œ ë°˜í™˜"""
    return os.path.join(
        base_dir,
        "ARTICLES",
        theme.upper(),
        f"year={today.year}",
        f"month={today.strftime('%m')}",
        f"day={today.strftime('%d')}"
    )

def load_json_files():
    """í…Œë§ˆë³„ JSON íŒŒì¼ ê²½ë¡œ ë¡œë“œ"""
    url_per_themes = {theme: [] for theme in THEMES}
    for theme in THEMES:
        folder_path = get_folder_path('json', theme)
        json_files = glob.glob(os.path.join(folder_path, '*.json'))
        url_per_themes[theme].extend(json_files)
    return url_per_themes

def extract_article_data(file_path):
    """JSON íŒŒì¼ì—ì„œ HTML íŒŒì‹± ë° ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        article = json.load(f)

    soup = BeautifulSoup(article['content'], "html.parser")
    article_data = {
        'title': '',
        'date': '',
        'authors': [],
        'url': article['url'],
        'text': ''
    }

    article_wrap = soup.find("div", class_=lambda c: c and "article-wrap" in c)
    if not article_wrap:
        print(f"âŒ 'article-wrap' ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URL: {article['url']}")
        return article_data

    # âœ… ì œëª© ì¶”ì¶œ
    cover_title = article_wrap.find("div", class_=lambda c: c and "cover-title" in c)
    article_data['title'] = cover_title.get_text(strip=True) if cover_title else ""

    # âœ… ì €ì ì¶”ì¶œ
    byline_author = article_wrap.find("div", class_=lambda c: c and "byline-attr-author" in c)
    if byline_author:
        try:
            author_links = byline_author.find_all("a")
            if author_links:
                article_data['authors'] = [a.get_text(strip=True) for a in author_links]
                if article_data['authors'] == ['']:
                    author_img = author_links[0].find('img')
                    article_data['authors'] = [author_img['alt'].split(', ')[0]]
            else:
                authors_text = byline_author.text
                article_data['authors'] = [author.strip() for author in authors_text.split(', ')] if ',' in authors_text else [authors_text.strip()]
        except Exception as e:
            print(f"âŒ 'authors' íŒŒì‹± ì‹¤íŒ¨: {e}. URL: {article_data['url']}")

    # âœ… ë‚ ì§œ ì¶”ì¶œ
    meta_time = article_wrap.find("time", class_=lambda c: c and "byline-attr-meta-time" in c)
    article_data['date'] = meta_time.get("data-timestamp", "") if meta_time else ""

    # âœ… ë³¸ë¬¸ ì¶”ì¶œ
    body_wrap = soup.find("div", class_=lambda c: c and "body-wrap" in c)
    if body_wrap:
        body_div = body_wrap.find("div", class_=lambda c: c and "body" in c)
        article_data['text'] = body_div.get_text(separator="\n", strip=True) if body_div else ""

    print(f"âœ… {article_data['url']} ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
    return article_data

def save_to_parquet(article_data, theme):
    """ê¸°ì‚¬ ë°ì´í„°ë¥¼ Parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    df = pd.DataFrame([article_data])  # ë‹¨ì¼ ê¸°ì‚¬ì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ DataFrame ìƒì„±

    title = article_data['url'].rsplit("/", 1)[-1].replace(".html", "").replace('-', '_')
    folder_name = get_folder_path('parquet', theme)

    os.makedirs(folder_name, exist_ok=True)
    parquet_file_path = os.path.join(folder_name, f"{title}.parquet")

    df.to_parquet(parquet_file_path, engine="pyarrow", compression="snappy", index=False)
    print(f"ğŸ¯ Parquet ì €ì¥ ì™„ë£Œ: {parquet_file_path}")

def process_article(theme, file_path):
    """JSON íŒŒì¼ì„ ì½ê³ , ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ í›„ Parquet ë³€í™˜"""
    article_data = extract_article_data(file_path)
    save_to_parquet(article_data, theme)

def process_articles(theme, file_paths):
    """í•œ ê°œì˜ í…Œë§ˆì— ì†í•œ ëª¨ë“  JSON íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬"""
    for file_path in file_paths:
        process_article(theme, file_path)


def process_articles_in_parallel(url_per_themes):
    """í…Œë§ˆ ë‹¨ìœ„ë¡œ ë³‘ë ¬ ì²˜ë¦¬"""
    max_workers = min(len(url_per_themes), os.cpu_count() // 2)  # í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜ ì œí•œ

    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_articles, theme, file_paths): theme
            for theme, file_paths in url_per_themes.items()
        }

        # âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        for future in concurrent.futures.as_completed(futures):
            try:
                future.result()  # ì˜ˆì™¸ ë°œìƒ ì‹œ ì¶œë ¥
            except Exception as e:
                print(f"âŒ í…Œë§ˆ '{futures[future]}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    url_per_themes = load_json_files()
    process_articles_in_parallel(url_per_themes)
