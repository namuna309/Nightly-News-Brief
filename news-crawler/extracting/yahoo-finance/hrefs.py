import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from webdriver_manager.chrome import ChromeDriverManager
import time

# Selenium ì„¤ì •
options = webdriver.ChromeOptions()
options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ
options.add_argument('--ignore-certificate-errors')
options.add_argument('--ignore-certificate-errors-spki-list')
options.add_argument('--ignore-ssl-errors')

# ChromeDriver ì‹¤í–‰
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# Yahoo Finance ë‰´ìŠ¤ í˜ì´ì§€ ì—´ê¸°
URL = "https://finance.yahoo.com/topic/stock-market-news/"
driver.get(URL)
time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

def extract_article_links():
    """
    - content í´ë˜ìŠ¤ë¥¼ í¬í•¨í•œ div ìš”ì†Œë“¤ì„ ì°¾ìŒ
    - í•´ë‹¹ div ë‚´ë¶€ì—ì„œ footer > publishing > i íƒœê·¸ ë°”ë¡œ ë‹¤ìŒì˜ í…ìŠ¤íŠ¸ í™•ì¸
    - í•´ë‹¹ í…ìŠ¤íŠ¸ê°€ 'yesterday'ë©´ content ë‚´ a íƒœê·¸ì˜ hrefë¥¼ ì¶”ì¶œí•˜ì—¬ ì¶œë ¥
    """
    try:
        content_divs = driver.find_elements(By.XPATH, "//div[contains(@class, 'content')]")
    except Exception as e:
        print("âŒ content ìš”ì†Œ ì°¾ê¸° ì‹¤íŒ¨:", e)
        return

    article_links = []

    for content in content_divs:
        try:
            # í•´ë‹¹ content ë‚´ë¶€ì—ì„œ ë‚ ì§œ ì •ë³´ ì°¾ê¸°
            date_element = content.find_element(By.XPATH, ".//div[contains(@class, 'footer')]//div[contains(@class, 'publishing')]")
            date_text = date_element.get_attribute("textContent").split("â€¢")[-1].strip()
        except Exception:
            continue  # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ

        if "yesterday" != date_text:
            try:
                # í•´ë‹¹ content ë‚´ë¶€ì˜ a íƒœê·¸ì—ì„œ href ì¶”ì¶œ
                article_link = content.find_element(By.XPATH, ".//a").get_attribute("href")
                article_links.append(article_link)
            except Exception:
                continue  # a íƒœê·¸ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ

    if article_links:
        print("ğŸ”¹ ì˜¤ëŠ˜ ê¸°ì‚¬ ë§í¬:")
        for link in article_links:
            print(link)
    else:
        print("ğŸš¨ 'yesterday'ì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ ë§í¬ ì—†ìŒ.")

# ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¬ë©´ì„œ 'yesterday' ì—¬ë¶€ë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
def scroll_down_until_yesterday(max_scrolls=20, wait_time=2):
    """
    - ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¬ë©´ì„œ 'publishing' í´ë˜ìŠ¤ë¥¼ í¬í•¨í•œ íƒœê·¸ ë‚´ i íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ í™•ì¸
    - 'yesterday'ê°€ í¬í•¨ë˜ë©´ ìŠ¤í¬ë¡¤ì„ ë©ˆì¶¤
    - ìµœëŒ€ max_scrolls íšŸìˆ˜ê¹Œì§€ ìŠ¤í¬ë¡¤
    """
    previous_date_texts = []
    for _ in range(max_scrolls):
        time.sleep(wait_time)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
        
        # XPathë¥¼ ì‚¬ìš©í•˜ì—¬ 'publishing' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ íƒœê·¸ ë‚´ë¶€ì˜ <i> íƒœê·¸ ì°¾ê¸°
        try:
            date_elements = driver.find_elements(By.XPATH, "//div[contains(@class, 'publishing')]")
            date_texts = [el.get_attribute("textContent").split("â€¢")[-1].strip() for el in date_elements]
        except Exception as e:
            print("âŒ XPath ì˜¤ë¥˜ ë°œìƒ:", e)
            return
        
        new_date_texts = date_texts[len(previous_date_texts):]
        for new_date_text in new_date_texts:
            print(new_date_text)
        print()
        ontains_number = any(re.search(r'\d', new_date_text) for new_date_text in new_date_texts)

        if not ontains_number:
            print("ğŸš¨ ë‚ ì§œ ì •ë³´ì— ìˆ«ìê°€ í¬í•¨ë˜ì§€ ì•ŠìŒ. ìŠ¤í¬ë¡¤ ì¤‘ë‹¨.")
            return  # ìˆ«ìê°€ ì—†ëŠ” ê²½ìš° í•¨ìˆ˜ ì¢…ë£Œ

        previous_date_texts.extend(new_date_texts)
        # 'yesterday'ê°€ ì—†ìœ¼ë©´ ìŠ¤í¬ë¡¤ ë‹¤ìš´
        driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
        print(f"â¬‡ ìŠ¤í¬ë¡¤ {_ + 1}íšŒ ì‹¤í–‰")

    print("ğŸš¨ ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜ ë„ë‹¬, ì¤‘ì§€")


# 'yesterday'ê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ìŠ¤í¬ë¡¤ ì‹¤í–‰
scroll_down_until_yesterday(max_scrolls=20, wait_time=2)

# ê²°ê³¼ ì¶œë ¥
print("ğŸ“Œ ìŠ¤í¬ë¡¤ ì™„ë£Œ. ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì‹œì‘")

# ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ ì‹¤í–‰
extract_article_links()


driver.quit()